---
layout: post
title:  "일일 알고리즘 체크"
date:   2025-03-25
excerpt: ""
tag:
- Algorithm
comments: false
---

### 1. MapReduce 알고리즘

개요:  
MapReduce는 구글이 개발한 분산 컴퓨팅을 위한 핵심 빅데이터 처리 알고리즘입니다. 여러 대의 컴퓨터에서 대규모 데이터를 효율적으로 처리하기 위해 계산을 두 가지 주요 단계로 나눕니다.

1. Map 단계:

- 입력 데이터를 키-값 쌍으로 분리합니다.
- 각 쌍을 독립적으로 처리합니다.
- 예: 문서에서 단어 발생 빈도를 세기 위해 단어를 키로, 빈도를 값으로 설정.  

2. Reduce 단계:

- Map 단계에서 나온 결과를 집계합니다.
- 빈도 합계나 리스트 병합 같은 계산을 수행합니다.
- 예: 모든 문서에서의 단어 발생 빈도를 합산.

AI 및 통계학에서의 활용:

- 머신러닝을 위한 대규모 데이터 전처리에 사용됩니다.
- 통계 계산(예: 평균, 분산)을 클러스터 전체에서 분산 처리하는 데 도움을 줍니다.
- 방대한 데이터셋에 대해 확장 가능한 AI 모델 훈련을 가능하게 합니다.

### 2. Bloom 필터 (Bloom Filter) 알고리즘

개요:  
Bloom 필터는 공간 효율적인 확률적 데이터 구조로, 집합에 특정 요소가 포함되어 있는지 여부를 빠르게 검사할 수 있습니다. 대신, **거짓 양성(false positive)**은 발생할 수 있지만 **거짓 음성(false negative)**은 없습니다.

동작 원리:

1. 여러 개의 해시 함수를 사용하여 입력 데이터를 여러 비트 위치로 매핑.
2. 데이터가 삽입될 때 해당 비트들을 1로 설정.
3. 특정 데이터가 존재하는지 확인할 때, 모든 비트가 1인지 검사.

AI 및 통계학에서의 활용:

- 대규모 데이터셋에서 중복 제거나 빠른 존재 여부 확인에 사용.
- 분산 시스템에서 데이터 노드 간 빠른 필터링 작업.
- 추천 시스템에서 사용자 히스토리 캐싱.

### 3. 해싱(Hashing) 알고리즘

개요:  
해싱은 데이터를 고정된 크기의 해시 값으로 변환하는 알고리즘입니다. 대규모 데이터에서 빠른 검색, 중복 제거, 데이터 분산 처리에 필수적입니다.

동작 원리:

1. 입력 데이터를 해시 함수에 넣어 고정 크기의 해시 값 생성.
2. 데이터 저장 시, 해시 값에 따라 특정 위치에 저장.
3. 검색 시에도 동일한 해시 함수를 통해 빠르게 데이터 위치 확인.

AI 및 통계학에서의 활용:

- 특성 해싱(Feature Hashing): 고차원 데이터를 낮은 차원으로 변환하여 메모리 절약.
- 데이터 샤딩: 대용량 데이터를 여러 서버에 균등하게 분산.
- 중복 데이터 검출: 동일한 데이터를 해시 값으로 빠르게 비교.

### 4. 샘플링(Sampling) 알고리즘

개요:  
샘플링 알고리즘은 대용량 데이터셋에서 대표성 있는 일부 데이터를 선택하여 전체 데이터를 추정하거나 분석하는 방법입니다. 처리 속도를 높이고, 메모리 사용량을 절감하는 데 필수적입니다.

주요 종류:

1. 랜덤 샘플링(Random Sampling)- 임의로 데이터를 선택.
2. 층화 샘플링(Stratified Sampling)- 데이터 집합을 그룹으로 나눈 뒤 각 그룹에서 샘플 추출.
3. 시스템 샘플링(Systematic Sampling)- 일정 간격으로 데이터를 선택.

AI 및 통계학에서의 활용:

- 모델 학습 시간 단축: 빅데이터 전체가 아닌 샘플만으로 학습 가능.
- 통계적 추정: 전체 데이터의 평균, 분포 등을 샘플로부터 예측.
- 데이터 시각화: 시각화 시 과도한 데이터로 인한 복잡도 감소.

### 5. k-평균(k-means) 클러스터링 알고리즘

개요:
k-평균 클러스터링은 데이터를 **k개의 군집(클러스터)**으로 나누는 비지도 학습 알고리즘입니다. 대규모 데이터셋에서 유사한 데이터끼리 묶는 데 효과적입니다.

동작 원리:

 1. 데이터에서 k개의 초기 중심(centroid) 설정.
 2. 각 데이터 포인트를 가장 가까운 중심에 할당.
 3. 각 클러스터의 평균으로 중심 업데이트.
 4. 변화가 없을 때까지 반복.

AI 및 통계학에서의 활용:

- 고객 세분화, 패턴 인식, 추천 시스템.
- 데이터 전처리 시 노이즈 제거.
- 차원 축소 후 군집화로 데이터 시각화에 활용.

### 6. Apriori 연관 규칙(Apriori Association Rule) 알고리즘

개요:
Apriori 알고리즘은 대규모 거래 데이터에서 항목 간의 연관 관계를 찾아내는 알고리즘입니다. 주로 **장바구니 분석(Market Basket Analysis)**에 사용됩니다.

동작 원리:

 1. 빈발 항목 집합(Frequent Itemsets) 탐색: 최소 지지도(minimum support) 이상인 항목 집합을 반복적으로 찾음.
 2. 연관 규칙 생성: 최소 신뢰도(minimum confidence)를 충족하는 규칙 도출.
 3. Apriori 원칙: 부분 집합이 빈발하지 않으면 전체 집합도 빈발하지 않음 → 탐색 공간 축소.

AI 및 통계학에서의 활용:

- 추천 시스템: 상품, 콘텐츠 추천.
- 이상 탐지: 비정상적인 항목 조합 발견.
- 데이터 마이닝: 패턴 인식, 고객 행동 분석.

### 7. 의사결정나무(Decision Tree) 알고리즘

개요:
의사결정나무는 데이터를 조건에 따라 분기하여 트리 형태로 분류하거나 회귀 예측하는 알고리즘입니다. 대규모 데이터셋에서도 직관적이며 빠르게 사용 가능합니다.

동작 원리:

 1. 데이터의 특성(feature) 중에서 정보 이득(Information Gain) 또는 **지니 지수(Gini Index)**가 가장 높은 특성을 기준으로 분할.
 2. 각 분할된 노드에서 다시 같은 과정을 반복.
 3. 리프 노드에 도달하면 최종 예측값 결정.

AI 및 통계학에서의 활용:

- 분류(Classification): 고객 이탈 예측, 질병 진단 등.
- 회귀(Regression): 가격 예측, 매출 예측.
- 특성 중요도 평가: 대규모 데이터에서 중요한 변수 찾기.

### 8. 랜덤 포레스트(Random Forest) 알고리즘

개요:
랜덤 포레스트는 여러 개의 **의사결정나무(Decision Tree)**를 결합하여 예측 성능을 높이는 앙상블 학습 알고리즘입니다. 대규모 데이터에서도 과적합(overfitting)을 방지하면서 높은 정확도를 제공합니다.

동작 원리:

 1. 데이터의 일부를 **부트스트랩 샘플링(Bootstrap Sampling)**으로 무작위 추출.
 2. 여러 개의 결정 트리를 독립적으로 학습.
 3. 분류 문제는 다수결 투표, 회귀 문제는 평균값으로 최종 결과 결정.

AI 및 통계학에서의 활용:

- 고차원 데이터에서 특성 중요도 평가.
- 대규모 분류/회귀 문제에 사용.
- 이상치 탐지 및 피처 선택에 효과적.

### 9. 그래디언트 부스팅(Gradient Boosting) 알고리즘

개요:
그래디언트 부스팅은 여러 약한 학습기(주로 의사결정나무)를 순차적으로 학습시키고, 이전 모델의 오차를 보완하는 방식으로 성능을 향상시키는 앙상블 학습 알고리즘입니다. 대규모 데이터에서 높은 예측력을 보여줍니다.

동작 원리:

 1. 초기 모델은 단순한 예측값 생성.
 2. 이전 모델의 **오차(Residual)**를 계산.
 3. 오차를 줄이는 방향으로 새로운 약한 학습기 추가.
 4. 여러 모델을 합산하여 최종 예측값 도출.

AI 및 통계학에서의 활용:

- **분류(Classification)**와 회귀(Regression) 문제 모두에 사용.
- 특성 중요도 파악, 이상치 감지, 대규모 데이터 분석.
- 대표 알고리즘: XGBoost, LightGBM, CatBoost 등.

### 10. 주성분 분석(PCA, Principal Component Analysis) 알고리즘

개요:
PCA는 고차원 데이터의 차원을 축소하면서도 최대한 많은 정보를 유지하려는 통계적 기법입니다. 데이터의 분산이 가장 큰 방향으로 주성분을 찾습니다.

주요 과정:

 1. 데이터 표준화.
 2. 공분산 행렬 계산.
 3. 고유값 분해로 주성분 축 찾기.
 4. 상위 몇 개 주성분 선택하여 차원 축소.

AI 및 통계학에서의 활용:

- 데이터 시각화: 2D/3D로 차원 축소.
- 특성 선택: 중요한 변수만 남겨 과적합 방지.
- 노이즈 제거 및 모델 학습 속도 향상.

### 11. LSH (Locality-Sensitive Hashing) 알고리즘

개요:
LSH는 고차원 데이터에서 유사한 항목끼리 빠르게 검색하기 위한 해싱 기반 알고리즘입니다. 특히 벡터 간의 유사도를 기준으로 데이터를 그룹화할 때 효과적입니다.

주요 특징:

- 유사한 입력은 같은 버킷(bucket)으로 해시될 가능성이 높음
- 유사도 기반 검색(예: 코사인 유사도, 유클리드 거리)에 최적화
- 정렬된 인덱스보다 훨씬 빠른 근사 최근접 이웃 검색(ANN)

활용 사례:

- 대규모 이미지/텍스트 유사도 검색
- 추천 시스템
- 중복 데이터 제거 및 클러스터링 전처리

### 12. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) 알고리즘

1. 개요
DBSCAN은 밀도 기반 클러스터링 알고리즘으로, 데이터의 밀도를 기준으로 클러스터를 형성합니다. 사전 군집 수를 정하지 않아도 되며, 이상치(노이즈)를 자동으로 식별할 수 있다는 특징이 있습니다.

2. 주요 개념 설명 (기초부터 차근차근)

- 클러스터(Cluster): 데이터가 밀집된 구역, 즉 서로 가까운 점들의 집합입니다.
- 이웃(Epsilon-이웃): 특정 거리(Epsilon) 안에 있는 데이터 포인트들입니다.
- 밀도(density): 이웃 안에 얼마나 많은 데이터 포인트가 있는지를 나타냅니다.
- 중심점(Core Point): 이웃 안에 최소한의 데이터 수(MinPts)가 있는 핵심 점입니다.
- 경계점(Border Point): 핵심점 이웃 안에 있지만, 자기 주변에는 MinPts를 만족하지 않는 점입니다.
- 노이즈(Noise): 어느 클러스터에도 속하지 않는 외딴 점입니다.

3. 작동 방식 (쉬운 순서로 정리)

    1. 임의의 점을 선택하여, 그 이웃(Epsilon 거리 안)에 있는 점들을 확인합니다.
    2. 이웃 점 개수가 MinPts 이상이면, 해당 점을 **클러스터의 중심(Core Point)**으로 설정하고 확장합니다.
    3. 이웃에 있는 점들 중 또 다른 Core Point가 있으면, 그 이웃도 계속 확장합니다.
    4. 이웃 수가 부족하면 그 점은 노이즈로 간주됩니다.
    5. 모든 점을 다 탐색할 때까지 반복합니다.

4. 장점

- 클러스터 수를 미리 몰라도 됨.
- 비정형적이고 복잡한 모양의 클러스터도 잘 탐지함.
- 이상치 탐지 기능 포함.

5. 단점

- Epsilon 거리와 MinPts 값을 잘 설정해야 함.
- 고차원 데이터에서는 거리 기반 밀도 계산이 어려워질 수 있음.

6. 활용 예시

- 고객 행동 데이터를 통한 이상 행동 탐지
- GPS 데이터를 활용한 지역 패턴 분석
- 비정형 데이터에서 군집 구조 자동 탐지

### 13. TF-IDF (Term Frequency–Inverse Document Frequency) 알고리즘

1. 개요

TF-IDF는 문서에서 단어의 중요도를 계산하는 데 사용되는 알고리즘입니다. 주로 텍스트 마이닝, 검색 엔진, 자연어 처리(NLP) 등에서 널리 활용됩니다.

2. 용어 설명 (기초부터 차근차근)

- TF (Term Frequency, 단어 빈도):
한 문서에서 특정 단어가 얼마나 자주 등장했는지를 나타냅니다.
→ 단어가 자주 등장할수록 그 문서에서 중요한 단어일 가능성이 높음.
- IDF (Inverse Document Frequency, 역문서 빈도):
여러 문서에서 특정 단어가 얼마나 드물게 나타나는지를 계산합니다.
→ 너무 많은 문서에서 자주 나타나는 단어(예: “그리고”, “또는”)는 중요하지 않다고 판단합니다.
- TF-IDF 계산 방법:
TF × IDF 값을 곱해서 하나의 가중치를 만듭니다.
이 값이 높을수록, 그 단어는 해당 문서에서 특별하고 중요한 단어임을 의미합니다.

3. 직관적 예시

- 문서 A에서 “인공지능”이라는 단어가 많이 등장하고, 다른 문서에서는 거의 안 쓰인다면 → TF-IDF 값이 높음
- 문서 A에서 “그리고”라는 단어가 많이 등장하고, 다른 문서에서도 흔히 보인다면 → TF-IDF 값이 낮음

4. 활용 예시

- 검색 엔진: 사용자가 입력한 검색어와 관련 있는 문서를 찾을 때 활용
- 문서 분류 및 클러스터링: 중요한 키워드를 기준으로 문서를 자동 분류
- 챗봇과 텍스트 요약 시스템: 핵심 문장 또는 단어 추출

5. 장점

- 단어 중요도를 정량적으로 평가할 수 있음
- 구현이 간단하고 직관적임
- 텍스트 데이터 전처리에서 핵심적인 역할

6. 단점

- 단어 간 문맥을 고려하지 않음 (ex. “강아지”와 “개”는 의미는 같지만 TF-IDF는 다르게 처리)
- 너무 짧거나 너무 긴 문서에서는 정확도가 떨어질 수 있음

### 14. 나이브 베이즈(Naive Bayes) 분류 알고리즘

1. 개요
나이브 베이즈는 확률 이론에 기반한 분류 알고리즘으로, 간단하지만 매우 강력합니다. 특히 텍스트 분류(예: 스팸 필터, 뉴스 분류)에서 많이 사용됩니다.

2. 핵심 원리
- 베이즈 정리(Bayes’ Theorem): 어떤 사건이 일어났을 때, 그 원인이 되었을 확률을 계산하는 공식입니다.
- 조건부 확률을 이용하여 입력 데이터가 어떤 분류에 속할 확률을 계산합니다.
- “나이브(naive)“라는 이름은 모든 특성(feature)이 서로 독립이라고 가정하기 때문에 붙은 것입니다.

3. 베이즈 정리 수식 설명
P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}
- P(C|X): X 특성을 가진 데이터가 클래스 C일 확률
- P(X|C): 클래스 C일 때 X 특성이 나타날 확률
- P(C): 클래스 C가 등장할 확률 (사전 확률)
- P(X): X 특성이 나타날 전체 확률

4. 나이브 베이즈 작동 방식
	1.	훈련 데이터로 각 클래스에 대한 확률(빈도 기반)을 계산
	2.	새로운 데이터가 주어지면, 각 클래스에 속할 확률을 계산
	3.	확률이 가장 높은 클래스를 선택

5. 장점
- 구현이 간단하고 계산 속도가 빠름
- 적은 양의 데이터로도 잘 작동
- 고차원 데이터에 강함 (예: 텍스트 데이터)

6. 단점
- 특성 간 독립성 가정이 현실과 다를 수 있음
- 연속형 변수 처리 시 추가적인 확률 분포 가정 필요

7. 활용 예시
- 이메일 스팸 분류기
- 감정 분석(긍정/부정 분류)
- 뉴스 기사 카테고리 분류

### 15. 단어 빈도 기반 피처 추출 — Bag of Words (BoW) 알고리즘

1. 개요
Bag of Words는 텍스트 데이터를 분석할 때, 문서에 어떤 단어가 얼마나 나왔는지를 수치화하여 머신러닝 모델이 처리할 수 있게 만드는 기법입니다. 문장 구조는 무시하고, 단어의 등장 빈도만 고려합니다.

2. 기본 개념
- 문서 전체에서 **고유 단어 목록(어휘 사전)**을 만듭니다.
- 각 문서를 벡터로 표현합니다. 이 벡터는 어휘 사전의 각 단어가 해당 문서에 몇 번 나왔는지를 나타냅니다.

예) 어휘 사전: [“빅데이터”, “통계”, “분석”]
문서 A: “빅데이터 통계 빅데이터”
→ BoW 벡터: [2, 1, 0]

3. 특징
- 문서 간 유사도를 계산하거나, 분류기에 입력값으로 사용합니다.
- 문장의 의미나 순서는 반영되지 않습니다.

4. 장점
- 구현이 간단하고 직관적입니다.
- 통계 기반 머신러닝에 잘 어울립니다.

5. 단점
- 단어의 위치나 문맥 정보를 반영하지 못합니다.
- 어휘가 많아질수록 벡터가 커지고 희소(sparse)해집니다.

6. 활용 예시
- 뉴스 기사나 블로그 글 분류
- 감정 분석(긍정/부정 판별)
- 키워드 기반 추천 시스템

### 16. 워드 임베딩(Word Embedding) 알고리즘

1. 개요
워드 임베딩은 단어를 컴퓨터가 이해할 수 있도록 숫자 벡터로 변환하는 기법입니다. 이 벡터들은 단어의 의미적 관계와 문맥을 반영하여 구성됩니다. 일반적인 단어 빈도 방식(Bag of Words)보다 훨씬 풍부한 의미를 담을 수 있어 **자연어 처리(NLP)**와 AI 분야에서 매우 중요합니다.

2. 왜 필요한가요?
- “강아지”와 “개”는 의미가 비슷하지만 BoW에서는 전혀 다르게 처리됩니다.
- 워드 임베딩은 의미적으로 가까운 단어들끼리 벡터 공간에서 가까운 위치에 위치하도록 학습합니다.

3. 대표적인 워드 임베딩 알고리즘
- Word2Vec: CBOW와 Skip-gram 방식이 있으며, 단어 주변 문맥을 이용해 학습합니다.
- GloVe (Global Vectors): 전체 말뭉치에서의 단어 동시 등장 행렬을 기반으로 학습합니다.
- FastText: 단어를 자소 단위(부분 단어)로 분해해 더 정교한 임베딩을 만듭니다.

4. 예시 (직관적으로)
- “왕 - 남자 + 여자 = 여왕”
→ 의미가 벡터 연산을 통해 유지됨 (단어 간 관계를 수치로 나타낼 수 있음)

5. 장점
- 문맥과 의미를 고려한 고차원 표현 가능
- 대규모 텍스트 학습에 강함
- 단어 간 유사성/관계 파악 가능

6. 단점
- 훈련에 많은 텍스트와 자원이 필요
- 희귀 단어는 제대로 학습되지 않을 수 있음
- 훈련 데이터의 편향이 임베딩에도 영향을 줄 수 있음

7. 활용 예시
- 감정 분석, 문서 분류, 번역 시스템
- 챗봇, 검색엔진, 추천 시스템
- 텍스트 기반 예측 모델의 입력 피처로 사용

### 스트리밍 데이터 처리(Stream Processing) 알고리즘

1. 개요
스트리밍 데이터 처리 알고리즘은 실시간 또는 거의 실시간으로 도착하는 데이터를 즉시 처리하는 기술입니다. 이는 주로 센서 데이터, 로그 데이터, SNS 데이터처럼 **계속해서 생성되는 데이터 흐름(stream)**에 사용됩니다.

2. 왜 중요한가요?
- 빅데이터 환경에서는 데이터를 저장해두고 나중에 처리하는 방식(Batch)이 비효율적일 수 있습니다.
- 예: 온라인 쇼핑몰에서 결제 사기 탐지, 실시간 주가 분석 등에서는 즉각적인 반응이 필요합니다.

3. 작동 방식
- 데이터가 들어오면, 대기 없이 즉시 연산(집계, 필터링, 변환 등)을 수행합니다.
- 데이터를 일시적으로 저장하는 **윈도우(window)**라는 개념을 이용하여 시간이나 이벤트 기준으로 처리합니다.
- 예: 5초 간격으로 평균값 계산, 최근 10개 트랜잭션 감시 등

4. 대표 알고리즘 및 시스템
- Apache Kafka + Apache Flink/Spark Streaming
- Sliding Window, Tumbling Window, Session Window
- CEP (Complex Event Processing): 여러 이벤트의 복잡한 패턴 감지

5. 장점
- 빠른 의사결정 가능 (거의 실시간 분석)
- 무한한 데이터 흐름에 적합
- 이상 탐지 및 경고 시스템에 활용 가능

6. 단점
- 실시간 처리를 위해 시스템 리소스가 더 많이 필요
- 지연(latency), 누락(missing data), 중복 문제 처리 필요
- 정밀한 분석보다는 빠른 대응에 초점

7. 활용 예시
- 실시간 교통 정보 분석 및 안내
- 실시간 광고 클릭 감지 및 전환율 분석
- IoT 센서 데이터 실시간 모니터링

### 18. k-최근접 이웃 (k-NN, k-Nearest Neighbors) 알고리즘

1. 개요
k-NN은 새로운 데이터가 등장했을 때, 기존 데이터 중 가장 가까운 k개의 이웃을 찾아, 다수결 투표 또는 평균을 통해 분류 또는 예측하는 알고리즘입니다.
초보자에게도 직관적인 방식으로 이해할 수 있는 대표적인 지도학습(Supervised Learning) 알고리즘입니다.

2. 핵심 원리
- 데이터를 공간상 점으로 생각하고, 거리를 기준으로 가까운 데이터를 찾습니다.
- “가깝다”는 기준은 주로 유클리드 거리를 사용합니다.
- k개의 이웃 중 가장 많은 클래스가 속한 클래스로 분류하거나, 평균값으로 회귀 예측을 수행합니다.

3. 작동 방식 (순서대로 정리)
	1.	예측하려는 새 데이터가 생기면
	2.	전체 훈련 데이터와의 거리를 모두 계산
	3.	거리가 가까운 순으로 k개를 선택
	4.	분류 문제: 가장 많이 등장한 클래스로 분류
	5.	회귀 문제: 평균값으로 예측

4. 장점
- 구조가 간단하고, 이해 및 구현이 쉬움
- 학습 자체는 거의 필요 없고, 저장만 해두면 바로 예측 가능
- 다양한 문제(분류, 회귀)에 적용 가능

5. 단점
- 예측 속도 느림 (모든 데이터를 비교해야 하므로)
- 고차원 데이터에 취약 (차원의 저주 문제 발생)
- k 값 설정이 성능에 큰 영향을 미침

6. 활용 예시
- 고객 성향 분류
- 질병 진단 (환자의 증상과 유사한 환자 기반 예측)
- 이미지 분류 (비슷한 이미지 기반 분류)

### 19. 이상치 탐지(Outlier Detection) 알고리즘

1. 개요
이상치 탐지 알고리즘은 데이터 중에서 **정상적인 패턴과는 다른 데이터(이상치)**를 찾아내는 데 사용됩니다. 빅데이터 분석, 통계학, 인공지능에서 이상치 제거 또는 이상 패턴 감지는 매우 중요한 과정입니다.

2. 왜 중요한가요?
- 이상치는 모델 성능을 악화시키거나 잘못된 예측을 유도할 수 있습니다.
- 예: 결제 사기, 장비 고장, 바이러스 감염 등은 모두 이상 행동입니다.

3. 주요 알고리즘 종류
- Z-Score 방식: 평균에서 얼마나 떨어져 있는지를 기준으로 판단
- IQR (사분위수) 방식: 상위/하위 극단값을 기준으로 판단
- LOF (Local Outlier Factor): 주변 데이터 밀도를 비교하여 이상치 판별
- Isolation Forest: 이상치는 다른 점들보다 쉽게 “격리”된다는 원리를 이용
- One-Class SVM: 정상 데이터의 경계를 학습한 후, 바깥에 있는 데이터는 이상치로 간주

4. 장점
- 데이터의 신뢰도를 높일 수 있음
- 보안, 품질 관리 등 다양한 분야에 활용 가능

5. 단점
- 어떤 데이터를 이상치로 볼지는 상황에 따라 다름
- 고차원일수록 정확한 이상치 탐지가 어려워짐

6. 활용 예시
- 금융 사기 탐지 시스템
- 제조 공정에서 이상 제품 감지
- 서버 트래픽 이상 모니터링
