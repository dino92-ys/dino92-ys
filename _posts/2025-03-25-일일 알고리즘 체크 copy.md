---
layout: post
title:  "일일 알고리즘 체크 Test"
date:   2025-03-25
excerpt: ""
tag:
- Algorithm
comments: false
---

### 1. MapReduce 알고리즘

개요:  
MapReduce는 구글이 개발한 분산 컴퓨팅을 위한 핵심 빅데이터 처리 알고리즘입니다. 여러 대의 컴퓨터에서 대규모 데이터를 효율적으로 처리하기 위해 계산을 두 가지 주요 단계로 나눕니다.

1. Map 단계:

- 입력 데이터를 키-값 쌍으로 분리합니다.
- 각 쌍을 독립적으로 처리합니다.
- 예: 문서에서 단어 발생 빈도를 세기 위해 단어를 키로, 빈도를 값으로 설정.  

2. Reduce 단계:

- Map 단계에서 나온 결과를 집계합니다.
- 빈도 합계나 리스트 병합 같은 계산을 수행합니다.
- 예: 모든 문서에서의 단어 발생 빈도를 합산.

AI 및 통계학에서의 활용:

- 머신러닝을 위한 대규모 데이터 전처리에 사용됩니다.
- 통계 계산(예: 평균, 분산)을 클러스터 전체에서 분산 처리하는 데 도움을 줍니다.
- 방대한 데이터셋에 대해 확장 가능한 AI 모델 훈련을 가능하게 합니다.

### 2. Bloom 필터 (Bloom Filter) 알고리즘

개요:  
Bloom 필터는 공간 효율적인 확률적 데이터 구조로, 집합에 특정 요소가 포함되어 있는지 여부를 빠르게 검사할 수 있습니다. 대신, **거짓 양성(false positive)**은 발생할 수 있지만 **거짓 음성(false negative)**은 없습니다.

동작 원리:

1. 여러 개의 해시 함수를 사용하여 입력 데이터를 여러 비트 위치로 매핑.
2. 데이터가 삽입될 때 해당 비트들을 1로 설정.
3. 특정 데이터가 존재하는지 확인할 때, 모든 비트가 1인지 검사.

AI 및 통계학에서의 활용:

- 대규모 데이터셋에서 중복 제거나 빠른 존재 여부 확인에 사용.
- 분산 시스템에서 데이터 노드 간 빠른 필터링 작업.
- 추천 시스템에서 사용자 히스토리 캐싱.

### 3. 해싱(Hashing) 알고리즘

개요:  
해싱은 데이터를 고정된 크기의 해시 값으로 변환하는 알고리즘입니다. 대규모 데이터에서 빠른 검색, 중복 제거, 데이터 분산 처리에 필수적입니다.

동작 원리:

1. 입력 데이터를 해시 함수에 넣어 고정 크기의 해시 값 생성.
2. 데이터 저장 시, 해시 값에 따라 특정 위치에 저장.
3. 검색 시에도 동일한 해시 함수를 통해 빠르게 데이터 위치 확인.

AI 및 통계학에서의 활용:

- 특성 해싱(Feature Hashing): 고차원 데이터를 낮은 차원으로 변환하여 메모리 절약.
- 데이터 샤딩: 대용량 데이터를 여러 서버에 균등하게 분산.
- 중복 데이터 검출: 동일한 데이터를 해시 값으로 빠르게 비교.

### 4. 샘플링(Sampling) 알고리즘

개요:  
샘플링 알고리즘은 대용량 데이터셋에서 대표성 있는 일부 데이터를 선택하여 전체 데이터를 추정하거나 분석하는 방법입니다. 처리 속도를 높이고, 메모리 사용량을 절감하는 데 필수적입니다.

주요 종류:

1. 랜덤 샘플링(Random Sampling)- 임의로 데이터를 선택.
2. 층화 샘플링(Stratified Sampling)- 데이터 집합을 그룹으로 나눈 뒤 각 그룹에서 샘플 추출.
3. 시스템 샘플링(Systematic Sampling)- 일정 간격으로 데이터를 선택.

AI 및 통계학에서의 활용:

- 모델 학습 시간 단축: 빅데이터 전체가 아닌 샘플만으로 학습 가능.
- 통계적 추정: 전체 데이터의 평균, 분포 등을 샘플로부터 예측.
- 데이터 시각화: 시각화 시 과도한 데이터로 인한 복잡도 감소.

### 5. k-평균(k-means) 클러스터링 알고리즘

개요:
k-평균 클러스터링은 데이터를 **k개의 군집(클러스터)**으로 나누는 비지도 학습 알고리즘입니다. 대규모 데이터셋에서 유사한 데이터끼리 묶는 데 효과적입니다.

동작 원리:

 1. 데이터에서 k개의 초기 중심(centroid) 설정.
 2. 각 데이터 포인트를 가장 가까운 중심에 할당.
 3. 각 클러스터의 평균으로 중심 업데이트.
 4. 변화가 없을 때까지 반복.

AI 및 통계학에서의 활용:

- 고객 세분화, 패턴 인식, 추천 시스템.
- 데이터 전처리 시 노이즈 제거.
- 차원 축소 후 군집화로 데이터 시각화에 활용.

### 6. Apriori 연관 규칙(Apriori Association Rule) 알고리즘

개요:
Apriori 알고리즘은 대규모 거래 데이터에서 항목 간의 연관 관계를 찾아내는 알고리즘입니다. 주로 **장바구니 분석(Market Basket Analysis)**에 사용됩니다.

동작 원리:

 1. 빈발 항목 집합(Frequent Itemsets) 탐색: 최소 지지도(minimum support) 이상인 항목 집합을 반복적으로 찾음.
 2. 연관 규칙 생성: 최소 신뢰도(minimum confidence)를 충족하는 규칙 도출.
 3. Apriori 원칙: 부분 집합이 빈발하지 않으면 전체 집합도 빈발하지 않음 → 탐색 공간 축소.

AI 및 통계학에서의 활용:

- 추천 시스템: 상품, 콘텐츠 추천.
- 이상 탐지: 비정상적인 항목 조합 발견.
- 데이터 마이닝: 패턴 인식, 고객 행동 분석.

### 7. 의사결정나무(Decision Tree) 알고리즘

개요:
의사결정나무는 데이터를 조건에 따라 분기하여 트리 형태로 분류하거나 회귀 예측하는 알고리즘입니다. 대규모 데이터셋에서도 직관적이며 빠르게 사용 가능합니다.

동작 원리:

 1. 데이터의 특성(feature) 중에서 정보 이득(Information Gain) 또는 **지니 지수(Gini Index)**가 가장 높은 특성을 기준으로 분할.
 2. 각 분할된 노드에서 다시 같은 과정을 반복.
 3. 리프 노드에 도달하면 최종 예측값 결정.

AI 및 통계학에서의 활용:

- 분류(Classification): 고객 이탈 예측, 질병 진단 등.
- 회귀(Regression): 가격 예측, 매출 예측.
- 특성 중요도 평가: 대규모 데이터에서 중요한 변수 찾기.

### 8. 랜덤 포레스트(Random Forest) 알고리즘

개요:
랜덤 포레스트는 여러 개의 **의사결정나무(Decision Tree)**를 결합하여 예측 성능을 높이는 앙상블 학습 알고리즘입니다. 대규모 데이터에서도 과적합(overfitting)을 방지하면서 높은 정확도를 제공합니다.

동작 원리:

 1. 데이터의 일부를 **부트스트랩 샘플링(Bootstrap Sampling)**으로 무작위 추출.
 2. 여러 개의 결정 트리를 독립적으로 학습.
 3. 분류 문제는 다수결 투표, 회귀 문제는 평균값으로 최종 결과 결정.

AI 및 통계학에서의 활용:

- 고차원 데이터에서 특성 중요도 평가.
- 대규모 분류/회귀 문제에 사용.
- 이상치 탐지 및 피처 선택에 효과적.

### 9. 그래디언트 부스팅(Gradient Boosting) 알고리즘

개요:
그래디언트 부스팅은 여러 약한 학습기(주로 의사결정나무)를 순차적으로 학습시키고, 이전 모델의 오차를 보완하는 방식으로 성능을 향상시키는 앙상블 학습 알고리즘입니다. 대규모 데이터에서 높은 예측력을 보여줍니다.

동작 원리:

 1. 초기 모델은 단순한 예측값 생성.
 2. 이전 모델의 **오차(Residual)**를 계산.
 3. 오차를 줄이는 방향으로 새로운 약한 학습기 추가.
 4. 여러 모델을 합산하여 최종 예측값 도출.

AI 및 통계학에서의 활용:

- **분류(Classification)**와 회귀(Regression) 문제 모두에 사용.
- 특성 중요도 파악, 이상치 감지, 대규모 데이터 분석.
- 대표 알고리즘: XGBoost, LightGBM, CatBoost 등.

### 10. 주성분 분석(PCA, Principal Component Analysis) 알고리즘

개요:
PCA는 고차원 데이터의 차원을 축소하면서도 최대한 많은 정보를 유지하려는 통계적 기법입니다. 데이터의 분산이 가장 큰 방향으로 주성분을 찾습니다.

주요 과정:

 1. 데이터 표준화.
 2. 공분산 행렬 계산.
 3. 고유값 분해로 주성분 축 찾기.
 4. 상위 몇 개 주성분 선택하여 차원 축소.

AI 및 통계학에서의 활용:

- 데이터 시각화: 2D/3D로 차원 축소.
- 특성 선택: 중요한 변수만 남겨 과적합 방지.
- 노이즈 제거 및 모델 학습 속도 향상.

### 11. LSH (Locality-Sensitive Hashing) 알고리즘

개요:
LSH는 고차원 데이터에서 유사한 항목끼리 빠르게 검색하기 위한 해싱 기반 알고리즘입니다. 특히 벡터 간의 유사도를 기준으로 데이터를 그룹화할 때 효과적입니다.

주요 특징:

- 유사한 입력은 같은 버킷(bucket)으로 해시될 가능성이 높음
- 유사도 기반 검색(예: 코사인 유사도, 유클리드 거리)에 최적화
- 정렬된 인덱스보다 훨씬 빠른 근사 최근접 이웃 검색(ANN)

활용 사례:

- 대규모 이미지/텍스트 유사도 검색
- 추천 시스템
- 중복 데이터 제거 및 클러스터링 전처리
